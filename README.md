# Audio Generation Showcase

This repository hosts a website showcasing the results of various audio generation models. The project demonstrates the capabilities of different architectures in generating instrumental tracks from vocal inputs.

You can view the live site here: [Audio Generation Showcase](https://utsahajoshi.github.io/music_gen_results)

## üéµ Models Showcased

1. **Independent GPT-2 Model (Standardized Less)**
2. **Independent GPT-2 Model (Standardized Full)**
3. **Joint Model (Standardized Full)**
4. **Cross Attention Model (Standardized Full)**

## üéº Structure for Each Model

Each model section contains three inferences, and each inference includes:

### Input

- **Vocal**: The vocal input used for conditioning the generation.
- **Reference Beat**: The reference beat used for guiding the generation of instrumental tracks.

### Generated Tracks (in order)

1. **Hi-Hat**
2. **Kick**
3. **Snare**
4. **Clap**
5. **Bass**
6. **Drums**
7. **Keys**
8. **Full Instrumental**

---

## üñ•Ô∏è How to View the Website

To view the website showcasing the models and their generated results:

1. **Visit the GitHub Pages URL**:
   Visit [this link](https://utsahajoshi.github.io/music_gen_results) to access the showcase.

2. **Model Pages**:
   Each model (Independent GPT-2, Joint, and Cross Attention models) has its own page containing the inferences and the corresponding audio results. You'll find sections such as:
   - **Inference 1, 2, and 3**: Showcasing the generation of various instrumental tracks.
   - **Input**: Providing the vocal and reference beat inputs.
   - **Generated**: Presenting the tracks like Hi-Hat, Kick, Snare, Clap, Bass, Drums, Keys, and Full Instrumental.
